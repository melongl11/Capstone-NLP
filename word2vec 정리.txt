Word2Vec
 - 단어의 벡터화. 실수 공간에 단어들을 벡터로 표현함
 - 실수 공간에 흩어져 있기 때문에 유사도를 측정할 수 있다.
  -> 그렇기 때문에 연산이 가능함. 
  -> ex) 컴퓨터_사람 = 전자기기, 한국-서울+파리=프랑스, 사랑+이별=추억
  -> ex2) biggest - big + small = smallest
  -> 각 벡터값을 계산한 후에, 결과값과 가장 가까운 벡터를 찾으면 위와 같은 결과가 나옴.
 - Assumption
  -> 비슷한 분포를 가진 단어들은 비슷한 의미를 가진다. => 단어들이 같은 문맥에서 등장하는가?
 - CBOW
  -> 문장의 빈칸을 채우는 것.
  -> 총 C개의 단어를 input으로 하여, 주어진 빈칸을 맞추기 위한 네트워크를 만드는 것.
  -> output은 1개의 단어.
 - Skip-gram
  -> CBOW와 반대방향의 모델.
  -> 현재 주어진 하나의 단어로 주위에 등장하는 나머지 몇 가지의 단어들의 등장 여부를 유추.
  -> input은 1개의 단어이고, output은 해당 단어와 유사도가 높은 단어들.
  -> 우리가 사용해야 하는 모델임.
